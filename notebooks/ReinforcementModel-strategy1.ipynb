{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 4 - Treasure Hunters Inc.\n",
    "###  We do the treasure hunting and monster fighting for you\n",
    "1. Set up a new git repository in your GitHub account\n",
    "2. Think up a map-like environment with treasure, obstacles and opponents\n",
    "3. Choose a programming language (Python, C/C++, Java)\n",
    "4. Formulate ideas on how reinforcement learning can be used to find treasure efficiently while avoiding obstacles and opponents\n",
    "5. Build one or more reinforcement policies to model situational assessments, actions and rewards programmatically\n",
    "6. Document your process and results\n",
    "7. Commit your source code, documentation and other supporting files to the git repository in GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Importing libraries\n",
    "- Here we only need numpy and copy for copying a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Defining the Environment for our model\n",
    "\n",
    "### Map :\n",
    "- Map is of 7rows and 7 columns\n",
    "- total of 49 possible states\n",
    "\n",
    "### Q-Values:\n",
    "- Initializing a 3 dimensional array with zeros to hold Q-values for each state and action pair Q(s,a)\n",
    "- This array consists of 7 rows and 7 columns and in 3rd dimension we have all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_rows = 7\n",
    "environment_columns = 7\n",
    "\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions:\n",
    "\n",
    "#### We have four possible actions for the agent:\n",
    "- up(value:0)\n",
    "- right(value:1)\n",
    "- down(value:2)\n",
    "- left(value:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Table:\n",
    "\n",
    "- Creating a two-dimensional reward table which can hold reward for every state\n",
    "- This reward table's shape matches our map's shape\n",
    "\n",
    "\n",
    "## Rewards\n",
    "\n",
    "- Let us punish with -50, if agent encounters its opponent\n",
    "- Let us punish with -30, if agent crashes into obstacle\n",
    "- Let us reward with 0, if agent finds treasure\n",
    "- Let us reward with 100, if agent reaches goal state\n",
    "- To motivate agent to finish quick, we shall punish every step with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPONENT_REWARD = -50\n",
    "TREASURE_REWARD = 0\n",
    "OBSTACLE_REWARD = -10\n",
    "EMPTY_REWARD = -1\n",
    "GOAL_REWARD= 100\n",
    "\n",
    "rewards = [\n",
    "            [EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, TREASURE_REWARD, OPPONENT_REWARD, OBSTACLE_REWARD],\n",
    "            [TREASURE_REWARD, OPPONENT_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, OPPONENT_REWARD],\n",
    "            [EMPTY_REWARD, EMPTY_REWARD, TREASURE_REWARD, EMPTY_REWARD, OBSTACLE_REWARD, EMPTY_REWARD, EMPTY_REWARD],\n",
    "            [EMPTY_REWARD, EMPTY_REWARD, OPPONENT_REWARD, EMPTY_REWARD, OBSTACLE_REWARD, EMPTY_REWARD, EMPTY_REWARD],\n",
    "            [EMPTY_REWARD, TREASURE_REWARD, EMPTY_REWARD, EMPTY_REWARD, OBSTACLE_REWARD, EMPTY_REWARD, EMPTY_REWARD],\n",
    "            [EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, TREASURE_REWARD, OBSTACLE_REWARD, EMPTY_REWARD, EMPTY_REWARD],\n",
    "            [EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, EMPTY_REWARD, GOAL_REWARD],\n",
    "        ]\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPPONENT = '_E_'\n",
    "TREASURE = '_$_'\n",
    "OBSTACLE = '|-|'\n",
    "EMPTY = '---'\n",
    "PLAYER = '_P_'\n",
    "PLAYERGOLD = '$P$'\n",
    "PLAYERGOAL = '^P^'\n",
    "GOAL= 'END'\n",
    "Visual = [\n",
    "            [EMPTY, EMPTY, EMPTY, EMPTY, TREASURE, OPPONENT, OBSTACLE],\n",
    "            [TREASURE, OPPONENT, EMPTY, EMPTY, EMPTY, EMPTY, OPPONENT],\n",
    "            [EMPTY, EMPTY, TREASURE, EMPTY, OBSTACLE, EMPTY, EMPTY],\n",
    "            [EMPTY, EMPTY, OPPONENT, EMPTY, OBSTACLE, EMPTY, EMPTY],\n",
    "            [EMPTY, TREASURE, EMPTY, EMPTY, OBSTACLE, EMPTY, EMPTY],\n",
    "            [EMPTY, EMPTY, EMPTY, TREASURE, OBSTACLE, EMPTY, EMPTY],\n",
    "            [EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, EMPTY, GOAL],\n",
    "        ]\n",
    "print(f\"Labels:::>\\n\\nOpponent: '{OPPONENT}'\\nTreasure: '{TREASURE}'\\nOBSTACLE: '{OBSTACLE}'\\nEmpty space: '{EMPTY}'\\n\")\n",
    "for row in Visual:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-3: Helper Functions:\n",
    "\n",
    "## To check if the current state is terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isTerminalState(current_row, current_column):\n",
    "    if rewards[current_row][current_column] == EMPTY_REWARD:\n",
    "        return False  \n",
    "    elif rewards[current_row][current_column] == TREASURE_REWARD:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To get starting location\n",
    "- Everytime we start our agent in the map from a random state which is not terminal\n",
    "- this is to help our agent to learn how to reach goal state from any non-terminal state after training.\n",
    "- loop until we find a non-terminal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStartingLocation():\n",
    "    current_row = np.random.randint(environment_rows)\n",
    "    current_column = np.random.randint(environment_columns)\n",
    "    while isTerminalState(current_row, current_column):\n",
    "        current_row = np.random.randint(environment_rows)\n",
    "        current_column = np.random.randint(environment_columns)\n",
    "    return current_row, current_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get next action to perform\n",
    "- if a randomly chosen value between 0 and 1 is less than epsilon, then choose the most promising value from the Q-table for this state. This is `Exploitation`\n",
    "- else we choose a random possible action. This is `Exploration`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextAction(current_row, current_column, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row][current_column])\n",
    "    else: #choose a random action\n",
    "        return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get next location\n",
    "- based on the action we chose, this function will return the next state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNextLocation(current_row, current_column, action_index):\n",
    "    new_row = current_row\n",
    "    new_column = current_column\n",
    "    if actions[action_index] == 'up' and current_row > 0:\n",
    "        new_row -= 1\n",
    "    elif actions[action_index] == 'right' and current_column < environment_columns - 1:\n",
    "        new_column += 1\n",
    "    elif actions[action_index] == 'down' and current_row < environment_rows - 1:\n",
    "        new_row += 1\n",
    "    elif actions[action_index] == 'left' and current_column > 0:\n",
    "        new_column -= 1\n",
    "    return new_row, new_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get optimal path:\n",
    "- This function returns the optimal path to goal, from the non-terminal start state that we choose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPath(start_row, start_column):\n",
    "    if isTerminalState(start_row, start_column):\n",
    "        return []\n",
    "    else: \n",
    "        current_row, current_column = start_row, start_column\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row, current_column])\n",
    "        while not isTerminalState(current_row, current_column):\n",
    "            action_index = getNextAction(current_row, current_column, 1.)\n",
    "            current_row, current_column = getNextLocation(current_row, current_column, action_index)\n",
    "            shortest_path.append([current_row, current_column])\n",
    "        return shortest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-4 : Training our agent\n",
    "- we are training on 10,000 episodes to get more optimal results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "#defining parameters\n",
    "#the percentage of time when we should take the best action (instead of a random action)\n",
    "epsilon = 0.9 \n",
    "#discount factor for future rewards\n",
    "discount_factor = 0.9 \n",
    "#the rate at which the agent should learn\n",
    "learning_rate = 0.9 \n",
    "\n",
    "\n",
    "for episode in range(10000):\n",
    "    row_index, column_index = getStartingLocation()\n",
    "\n",
    "    #continue moving until we reach a terminal state\n",
    "    while not isTerminalState(row_index, column_index):\n",
    "        \n",
    "        #choose which action to take\n",
    "        action_index = getNextAction(row_index, column_index, epsilon)\n",
    "\n",
    "        #transition to the next state and store the old row and column indexes\n",
    "        old_row_index, old_column_index = row_index, column_index \n",
    "        row_index, column_index = getNextLocation(row_index, column_index, action_index)\n",
    "\n",
    "        #receive the reward for moving to the new state\n",
    "        reward = rewards[row_index][column_index]\n",
    "        \n",
    "        # calculate the temporal difference\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "        #update the Q-value in q-table\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "print('Training completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step5: Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1], [2, 2], [2, 3], [3, 3], [4, 3], [5, 3], [6, 3], [6, 4], [6, 5], [6, 6]]\n",
      "[[0, 3], [0, 4], [1, 4], [1, 5], [2, 5], [2, 6], [3, 6], [4, 6], [5, 6], [6, 6]]\n"
     ]
    }
   ],
   "source": [
    "print(getPath(2, 1)) #starting at row 9, column 5\n",
    "print(getPath(0,3)) #starting at row 9, column 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step6: Visualizing agent path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeRoute(path,track=Visual):\n",
    "    \n",
    "    print(f\"::::Labels:::>\\n\\nOpponent: '{OPPONENT}'\\nTreasure: '{TREASURE}'\\nOBSTACLE: '{OBSTACLE}'\\nEmpty   : '{EMPTY}'\")\n",
    "    print(f\"Agent  : '{PLAYER}'\")    \n",
    "    print(f\"Agent with treasure : '{PLAYERGOLD}'\")\n",
    "    print(f\"Agent reached goal : '{PLAYERGOAL}'\")\n",
    "    print(\"\\nInitial Map\")\n",
    "    for row in Visual:\n",
    "        print(row)\n",
    "    print(f'Result:\\nstarting point: row->{path[0][0]+1} column->{path[0][1]+1}')\n",
    "    tVisual=copy.deepcopy(Visual)\n",
    "    goldCount=0\n",
    "    for step in path:\n",
    "        if tVisual[step[0]][step[1]]==EMPTY:\n",
    "            tVisual[step[0]][step[1]]=PLAYER\n",
    "        elif tVisual[step[0]][step[1]]==TREASURE:\n",
    "            tVisual[step[0]][step[1]]=PLAYERGOLD\n",
    "            goldCount+=1\n",
    "        elif tVisual[step[0]][step[1]]==GOAL:\n",
    "            tVisual[step[0]][step[1]]=PLAYERGOAL\n",
    "        else:\n",
    "            print('ERROR')\n",
    "    for row in tVisual:\n",
    "        print(row)\n",
    "    print(f'total steps: {len(path)}')\n",
    "    print(f'total treasure collected {goldCount}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimal path if started from 3rd row and 2nd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::::Labels:::>\n",
      "\n",
      "Opponent: '_E_'\n",
      "Treasure: '_$_'\n",
      "OBSTACLE: '|-|'\n",
      "Empty   : '---'\n",
      "Agent  : '_P_'\n",
      "Agent with treasure : '$P$'\n",
      "Agent reached goal : '^P^'\n",
      "\n",
      "Initial Map\n",
      "['---', '---', '---', '---', '_$_', '_E_', '|-|']\n",
      "['_$_', '_E_', '---', '---', '---', '---', '_E_']\n",
      "['---', '---', '_$_', '---', '|-|', '---', '---']\n",
      "['---', '---', '_E_', '---', '|-|', '---', '---']\n",
      "['---', '_$_', '---', '---', '|-|', '---', '---']\n",
      "['---', '---', '---', '_$_', '|-|', '---', '---']\n",
      "['---', '---', '---', '---', '---', '---', 'END']\n",
      "Result:\n",
      "starting point: row->3 column->2\n",
      "['---', '---', '---', '---', '_$_', '_E_', '|-|']\n",
      "['_$_', '_E_', '---', '---', '---', '---', '_E_']\n",
      "['---', '_P_', '$P$', '_P_', '|-|', '---', '---']\n",
      "['---', '---', '_E_', '_P_', '|-|', '---', '---']\n",
      "['---', '_$_', '---', '_P_', '|-|', '---', '---']\n",
      "['---', '---', '---', '$P$', '|-|', '---', '---']\n",
      "['---', '---', '---', '_P_', '_P_', '_P_', '^P^']\n",
      "total steps: 10\n",
      "total treasure collected 2\n"
     ]
    }
   ],
   "source": [
    "visualizeRoute(getPath(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimal path if started from 1st row and 4th column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::::Labels:::>\n",
      "\n",
      "Opponent: '_E_'\n",
      "Treasure: '_$_'\n",
      "OBSTACLE: '|-|'\n",
      "Empty   : '---'\n",
      "Agent  : '_P_'\n",
      "Agent with treasure : '$P$'\n",
      "Agent reached goal : '^P^'\n",
      "\n",
      "Initial Map\n",
      "['---', '---', '---', '---', '_$_', '_E_', '|-|']\n",
      "['_$_', '_E_', '---', '---', '---', '---', '_E_']\n",
      "['---', '---', '_$_', '---', '|-|', '---', '---']\n",
      "['---', '---', '_E_', '---', '|-|', '---', '---']\n",
      "['---', '_$_', '---', '---', '|-|', '---', '---']\n",
      "['---', '---', '---', '_$_', '|-|', '---', '---']\n",
      "['---', '---', '---', '---', '---', '---', 'END']\n",
      "Result:\n",
      "starting point: row->1 column->4\n",
      "['---', '---', '---', '_P_', '$P$', '_E_', '|-|']\n",
      "['_$_', '_E_', '---', '---', '_P_', '_P_', '_E_']\n",
      "['---', '---', '_$_', '---', '|-|', '_P_', '_P_']\n",
      "['---', '---', '_E_', '---', '|-|', '---', '_P_']\n",
      "['---', '_$_', '---', '---', '|-|', '---', '_P_']\n",
      "['---', '---', '---', '_$_', '|-|', '---', '_P_']\n",
      "['---', '---', '---', '---', '---', '---', '^P^']\n",
      "total steps: 10\n",
      "total treasure collected 1\n"
     ]
    }
   ],
   "source": [
    "visualizeRoute(getPath(0,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the optimal path if started from 1st row and 1stcolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "::::Labels:::>\n",
      "\n",
      "Opponent: '_E_'\n",
      "Treasure: '_$_'\n",
      "OBSTACLE: '|-|'\n",
      "Empty   : '---'\n",
      "Agent  : '_P_'\n",
      "Agent with treasure : '$P$'\n",
      "Agent reached goal : '^P^'\n",
      "\n",
      "Initial Map\n",
      "['---', '---', '---', '---', '_$_', '_E_', '|-|']\n",
      "['_$_', '_E_', '---', '---', '---', '---', '_E_']\n",
      "['---', '---', '_$_', '---', '|-|', '---', '---']\n",
      "['---', '---', '_E_', '---', '|-|', '---', '---']\n",
      "['---', '_$_', '---', '---', '|-|', '---', '---']\n",
      "['---', '---', '---', '_$_', '|-|', '---', '---']\n",
      "['---', '---', '---', '---', '---', '---', 'END']\n",
      "Result:\n",
      "starting point: row->1 column->1\n",
      "['_P_', '---', '---', '---', '_$_', '_E_', '|-|']\n",
      "['$P$', '_E_', '---', '---', '---', '---', '_E_']\n",
      "['_P_', '_P_', '$P$', '_P_', '|-|', '---', '---']\n",
      "['---', '---', '_E_', '_P_', '|-|', '---', '---']\n",
      "['---', '_$_', '---', '_P_', '|-|', '---', '---']\n",
      "['---', '---', '---', '$P$', '|-|', '---', '---']\n",
      "['---', '---', '---', '_P_', '_P_', '_P_', '^P^']\n",
      "total steps: 13\n",
      "total treasure collected 3\n"
     ]
    }
   ],
   "source": [
    "visualizeRoute(getPath(0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Future Developments:\n",
    "\n",
    "- Our model performed best with the current reward policy we have good results in the end.\n",
    "- In future we can try to move the enemy everytime and agent has to escape enemy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
